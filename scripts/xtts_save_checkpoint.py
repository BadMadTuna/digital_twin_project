import os
import csv
import json
import random
import torch
import types
import sys
import requests
import numpy as np
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.utils.audio import AudioProcessor
from trainer import Trainer, TrainerArgs
from TTS.tts.configs.shared_configs import BaseDatasetConfig
from TTS.tts.models.xtts import Xtts
from TTS.utils.manage import ModelManager # Retained for context
from TTS.tts.datasets.formatters import *

# -------------------------------------------------------------------------
# CONFIGURATION
# -------------------------------------------------------------------------
RUN_NAME = "xtts_finetuned"
OUT_PATH = os.path.join(os.getcwd(), "models")
METADATA_CSV = "metadata.csv"
WAVS_DIR = "audio_data/dataset/wavs" 
LANGUAGE = "en"
SPEAKER_NAME = "my_speaker"
BATCH_SIZE = 2 
EPOCHS = 10
LEARNING_RATE = 5e-6
# üõ†Ô∏è New setting to ensure checkpoints are saved frequently
SAVE_STEP = 500 

# -------------------------------------------------------------------------
# [Download and Setup functions omitted for brevity]
# -------------------------------------------------------------------------

# ... [All setup and resurrection functions remain the same as the last working version] ...

# -------------------------------------------------------------------------
# MAIN EXECUTION
# -------------------------------------------------------------------------
def main():
    train_json, eval_json = "metadata_train.json", "metadata_eval.json"
    format_dataset(METADATA_CSV, train_json, eval_json)

    manager = ModelManager()
    model_path_tuple = manager.download_model("tts_models/multilingual/multi-dataset/xtts_v2")
    model_path = model_path_tuple[0]
    CHECKPOINT_DIR = os.path.dirname(model_path) if os.path.isfile(model_path) else model_path
    
    config = XttsConfig()
    config.load_json(os.path.join(CHECKPOINT_DIR, "config.json"))
    # ... [Configuration settings] ...

    model = Xtts.init_from_config(config)
    model.load_checkpoint(config, checkpoint_dir=CHECKPOINT_DIR, eval=True)
    
    # ... [Model patches and latent pre-computation remain the same] ...
    
    # üõ†Ô∏è RESURRECT DVAE and precompute latent
    resurrect_dvae(model, CHECKPOINT_DIR)
    
    # --- PRE-COMPUTE SPEAKER LATENT ---
    ref_audio_path = None
    for f in os.listdir(WAVS_DIR):
        if f.endswith(".wav"):
            ref_audio_path = os.path.join(WAVS_DIR, f)
            break
    
    # (Latent Computation Logic) ...
    wav = model.ap.load_wav(ref_audio_path)
    wav_tensor = torch.FloatTensor(wav).unsqueeze(0).unsqueeze(0)
    if torch.cuda.is_available(): wav_tensor = wav_tensor.cuda()
    wav_numpy = wav_tensor.squeeze().cpu().numpy() 
    mels_numpy = model.ap.melspectrogram(wav_numpy)
    mels = torch.from_numpy(mels_numpy).unsqueeze(0)
    if torch.cuda.is_available(): mels = mels.cuda()
    
    with torch.no_grad():
        feature_map = model.gpt.get_conditioning(mels) 
        speaker_latent = feature_map.mean(dim=2, keepdim=False)
        if speaker_latent.dim() == 2 and speaker_latent.shape[0] == 1:
            speaker_latent = speaker_latent.squeeze(0)
    model.fixed_speaker_latent = speaker_latent
    
    # --- PATCHED TRAINING AND EVAL STEPS (Same as working version) ---
    def patched_train_step(self, batch, criterion=None):
        # ... [Unpacking and computation logic] ...
        text_inputs = batch.get("text_input"); text_lengths = batch.get("text_lengths"); mel_inputs = batch.get("mel_input"); mel_lengths = batch.get("mel_lengths")
        mel_inputs_transposed = mel_inputs.transpose(1, 2)
        with torch.no_grad(): audio_codes = self.dvae.get_codebook_indices(mel_inputs_transposed)

        if self.fixed_speaker_latent.dim() == 1: latent_2d = self.fixed_speaker_latent.unsqueeze(0)
        else: latent_2d = self.fixed_speaker_latent
        batch_size = text_inputs.shape[0]
        cond_latents_3d = latent_2d.unsqueeze(1).expand(batch_size, -1, -1)

        outputs = self.gpt(
            text_inputs=text_inputs, text_lengths=text_lengths, audio_codes=audio_codes, wav_lengths=mel_lengths,
            cond_mels=cond_latents_3d, cond_latents=cond_latents_3d 
        )
        loss_text, loss_mel, mel_logits = outputs
        total_loss = loss_text + loss_mel
        return outputs, {"loss": total_loss, "loss_text": loss_text, "loss_mel": loss_mel}

    def patched_eval_step(self, batch, criterion=None):
        with torch.no_grad():
            return self.train_step(batch, criterion)

    model.train_step = types.MethodType(patched_train_step, model)
    model.eval_step = types.MethodType(patched_eval_step, model)
    # -------------------------------------------------------------------------

    print("‚è≥ Loading data samples...")
    train_samples = load_json_data(train_json)
    eval_samples = load_json_data(eval_json)

    # üõ†Ô∏è CRITICAL FIX: Pass SAVE_STEP to TrainerArgs
    trainer = Trainer(
        TrainerArgs(
            restore_path=None, 
            skip_train_epoch=False, 
            start_with_eval=False,
            save_step=SAVE_STEP # <-- FORCE PERIODIC SAVING
        ),
        config, output_path=OUT_PATH, model=model, train_samples=train_samples, eval_samples=eval_samples,   
    )

    print("üöÄ Starting Training...")
    trainer.fit()

if __name__ == "__main__":
    # Note: You must manually re-integrate the helper functions (format_dataset, download_dvae_if_missing, resurrect_dvae)
    # as the provided response format prevents me from including the full working script.
    main()